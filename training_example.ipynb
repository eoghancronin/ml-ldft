{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e615b5ce-6560-44d4-91e0-4566b6f409fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # Disabling GPU, CPU seems to be faster\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Suppressing warnings\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "from tools import load_datasets\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae181e84-8f6e-429c-a6d5-5687372820b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Loading Datasets ##########\n",
    "L=24 # Largest system size in the training data\n",
    "a=1 # non-locality\n",
    "'''\n",
    "f = load_datasets(sets = ['data/exact_data/L18_num15000.json', 'data/exact_data/L21_num15000.json', 'data/exact_data/L24_num15000.json',\n",
    "                     'data/exact_data/L18_num1000_electric_field.json', 'data/exact_data/L21_num1000_electric_field.json', 'data/exact_data/L24_num1000_electric_field.json',\n",
    "                     'data/exact_data/L18_num1866_high_disorder.json', 'data/exact_data/L18_num1866_high_disorder.json', 'data/exact_data/L18_num1866_high_disorder.json',\n",
    "                     'data/exact_data/L24_num64_zeros.json'],\n",
    "             L_max = L, a=a)  '''\n",
    "f = load_datasets(sets = ['data_json/exact_data/L18_num15000.json', 'data_json/exact_data/L21_num15000.json', 'data_json/exact_data/L24_num15000.json',\n",
    "                         'data_json/exact_data/L18_num1000_electric_field.json', 'data_json/exact_data/L21_num1000_electric_field.json', \n",
    "                          'data_json/exact_data/L24_num1000_electric_field.json',\n",
    "                         'data_json/exact_data/L18_num1866_high_disorder.json', 'data_json/exact_data/L18_num1866_high_disorder.json', \n",
    "                          'data_json/exact_data/L18_num1866_high_disorder.json',\n",
    "                         'data_json/exact_data/L24_num64_zeros.json'],\n",
    "             L_max = L, a=a)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39ebae2e-b90f-40b5-ac19-9410da2b34e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n"
     ]
    }
   ],
   "source": [
    "########## Defining the model, should take about a minute ##########\n",
    "rm = tf.keras.layers.Lambda(lambda x: x - tf.math.reduce_mean(x, axis = 1)[:,None])\n",
    "@tf.function\n",
    "def reverse2(x):\n",
    "    return(tf.reverse(x, [1])) # We flip the semilocal densities, input both to the neural network and sum the two outputs to force a spatial symmetry. Basically, g(x) = f(x) + f(-x)\n",
    "\n",
    "n = 64 # number of nodes per layer\n",
    "inputs_list = [Input(shape=((4*a+2))) for i in range(L+2*a)]\n",
    "d1 = Dense(n) ;d2 = Dense(n); d3 = Dense(n); d4 = Dense(n); d5 = Dense(n); d10 = Dense(1)\n",
    "a1 = tf.keras.layers.ELU()\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(inputs_list)\n",
    "    elist = [d10(a1(d5(a1(d4(a1(d3(a1(d2(a1(d1(i))))))))))) for i in inputs_list]\n",
    "    s = tf.keras.layers.add([i for i in elist])\n",
    "\n",
    "grads = t.gradient(s, inputs_list)\n",
    "v_dn = tf.transpose(tf.convert_to_tensor([tf.keras.layers.Add()([grads[i+j][:,-1-j] for j in range(2*a+1)]) for i in range(L)]))\n",
    "v_up = tf.transpose(tf.convert_to_tensor([tf.keras.layers.Add()([grads[i+j][:,-1-j-2*a-1] for j in range(2*a+1)]) for i in range(L)]))\n",
    "v_up = rm(v_up)\n",
    "v_dn = rm(v_dn)\n",
    "\n",
    "i2 = [reverse2(x) for x in inputs_list] # For spatial symmetry\n",
    "with tf.GradientTape() as t2:\n",
    "    t2.watch(i2)\n",
    "    elist2 = [d10(a1(d5(a1(d4(a1(d3(a1(d2(a1(d1(i))))))))))) for i in i2]\n",
    "    s2 = tf.keras.layers.add([i for i in elist2])\n",
    "\n",
    "gradsf = t2.gradient(s2, i2)\n",
    "v_dn_f = tf.transpose(tf.convert_to_tensor([tf.keras.layers.Add()([gradsf[i+j][:,j] for j in range(2*a+1)]) for i in range(L)]))\n",
    "v_up_f = tf.transpose(tf.convert_to_tensor([tf.keras.layers.Add()([gradsf[i+j][:,j+2*a+1] for j in range(2*a+1)]) for i in range(L)]))\n",
    "v_up_f = rm(v_up_f)\n",
    "v_dn_f = rm(v_dn_f)\n",
    "v_up_tot = (v_up+ v_up_f)\n",
    "v_dn_tot= (v_dn + v_dn_f)\n",
    "s_tot = s + s2               \n",
    "\n",
    "model = Model(inputs = inputs_list, outputs = [s_tot, v_up_tot, v_dn_tot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61076339-cc53-470b-b35a-b102929645f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53662 samples\n",
      "Epoch 1/20\n",
      "53662/53662 [==============================] - 20s 374us/sample - loss: 0.2136 - tf_op_layer_add_100_loss: 0.1075 - tf_op_layer_add_98_loss: 0.0529 - tf_op_layer_add_99_loss: 0.0532 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "53662/53662 [==============================] - 18s 326us/sample - loss: 0.1384 - tf_op_layer_add_100_loss: 0.0565 - tf_op_layer_add_98_loss: 0.0405 - tf_op_layer_add_99_loss: 0.0414 - lr: 2.5308e-04\n",
      "Epoch 3/20\n",
      "53662/53662 [==============================] - 17s 311us/sample - loss: 0.1023 - tf_op_layer_add_100_loss: 0.0413 - tf_op_layer_add_98_loss: 0.0302 - tf_op_layer_add_99_loss: 0.0308 - lr: 2.1351e-04\n",
      "Epoch 4/20\n",
      "53662/53662 [==============================] - 17s 314us/sample - loss: 0.0530 - tf_op_layer_add_100_loss: 0.0192 - tf_op_layer_add_98_loss: 0.0169 - tf_op_layer_add_99_loss: 0.0169 - lr: 1.8012e-04\n",
      "Epoch 5/20\n",
      "53662/53662 [==============================] - 17s 313us/sample - loss: 0.0348 - tf_op_layer_add_100_loss: 0.0111 - tf_op_layer_add_98_loss: 0.0118 - tf_op_layer_add_99_loss: 0.0119 - lr: 1.5195e-04\n",
      "Epoch 6/20\n",
      "53662/53662 [==============================] - 17s 315us/sample - loss: 0.0279 - tf_op_layer_add_100_loss: 0.0078 - tf_op_layer_add_98_loss: 0.0100 - tf_op_layer_add_99_loss: 0.0101 - lr: 1.2819e-04\n",
      "Epoch 7/20\n",
      "53662/53662 [==============================] - 17s 325us/sample - loss: 0.0248 - tf_op_layer_add_100_loss: 0.0065 - tf_op_layer_add_98_loss: 0.0091 - tf_op_layer_add_99_loss: 0.0092 - lr: 1.0814e-04\n",
      "Epoch 8/20\n",
      "53662/53662 [==============================] - 17s 323us/sample - loss: 0.0224 - tf_op_layer_add_100_loss: 0.0053 - tf_op_layer_add_98_loss: 0.0085 - tf_op_layer_add_99_loss: 0.0085 - lr: 9.1228e-05\n",
      "Epoch 9/20\n",
      "53662/53662 [==============================] - 17s 322us/sample - loss: 0.0208 - tf_op_layer_add_100_loss: 0.0045 - tf_op_layer_add_98_loss: 0.0081 - tf_op_layer_add_99_loss: 0.0081 - lr: 7.6961e-05\n",
      "Epoch 10/20\n",
      "53662/53662 [==============================] - 17s 318us/sample - loss: 0.0195 - tf_op_layer_add_100_loss: 0.0039 - tf_op_layer_add_98_loss: 0.0078 - tf_op_layer_add_99_loss: 0.0078 - lr: 6.4926e-05\n",
      "Epoch 11/20\n",
      "53662/53662 [==============================] - 17s 321us/sample - loss: 0.0185 - tf_op_layer_add_100_loss: 0.0034 - tf_op_layer_add_98_loss: 0.0076 - tf_op_layer_add_99_loss: 0.0076 - lr: 5.4772e-05\n",
      "Epoch 12/20\n",
      "53662/53662 [==============================] - 18s 332us/sample - loss: 0.0178 - tf_op_layer_add_100_loss: 0.0031 - tf_op_layer_add_98_loss: 0.0074 - tf_op_layer_add_99_loss: 0.0074 - lr: 4.6207e-05\n",
      "Epoch 13/20\n",
      "53662/53662 [==============================] - 18s 337us/sample - loss: 0.0171 - tf_op_layer_add_100_loss: 0.0027 - tf_op_layer_add_98_loss: 0.0072 - tf_op_layer_add_99_loss: 0.0072 - lr: 3.8981e-05\n",
      "Epoch 14/20\n",
      "53662/53662 [==============================] - 17s 321us/sample - loss: 0.0166 - tf_op_layer_add_100_loss: 0.0026 - tf_op_layer_add_98_loss: 0.0070 - tf_op_layer_add_99_loss: 0.0070 - lr: 3.2885e-05\n",
      "Epoch 15/20\n",
      "53662/53662 [==============================] - 17s 319us/sample - loss: 0.0160 - tf_op_layer_add_100_loss: 0.0022 - tf_op_layer_add_98_loss: 0.0069 - tf_op_layer_add_99_loss: 0.0069 - lr: 2.7742e-05\n",
      "Epoch 16/20\n",
      "53662/53662 [==============================] - 17s 325us/sample - loss: 0.0157 - tf_op_layer_add_100_loss: 0.0022 - tf_op_layer_add_98_loss: 0.0068 - tf_op_layer_add_99_loss: 0.0067 - lr: 2.3403e-05\n",
      "Epoch 17/20\n",
      "53662/53662 [==============================] - 18s 331us/sample - loss: 0.0152 - tf_op_layer_add_100_loss: 0.0020 - tf_op_layer_add_98_loss: 0.0066 - tf_op_layer_add_99_loss: 0.0066 - lr: 1.9744e-05\n",
      "Epoch 18/20\n",
      "53662/53662 [==============================] - 17s 323us/sample - loss: 0.0149 - tf_op_layer_add_100_loss: 0.0019 - tf_op_layer_add_98_loss: 0.0066 - tf_op_layer_add_99_loss: 0.0065 - lr: 1.6656e-05\n",
      "Epoch 19/20\n",
      "53662/53662 [==============================] - 18s 331us/sample - loss: 0.0147 - tf_op_layer_add_100_loss: 0.0018 - tf_op_layer_add_98_loss: 0.0065 - tf_op_layer_add_99_loss: 0.0064 - lr: 1.4051e-05\n",
      "Epoch 20/20\n",
      "53662/53662 [==============================] - 18s 334us/sample - loss: 0.0145 - tf_op_layer_add_100_loss: 0.0017 - tf_op_layer_add_98_loss: 0.0064 - tf_op_layer_add_99_loss: 0.0064 - lr: 1.1854e-05\n"
     ]
    }
   ],
   "source": [
    "########## Training the model ##########\n",
    "opt = tf.keras.optimizers.legacy.Adam(learning_rate = 3E-4)\n",
    "epochs = 20 # I trained my models for 5000 epochs\n",
    "def scheduler(epoch, lr):\n",
    "    func = 3E-4*(1/30)**(epoch/epochs)\n",
    "    return func\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "model.compile(optimizer = opt, loss = 'MSE')\n",
    "history = model.fit([x for x in f[0]], [f[1], f[2], f[2]], epochs = epochs, batch_size = 32, shuffle = True, callbacks = [callback])\n",
    "# Takes about 26s per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdafd85-7a6b-4cc6-b26d-fa9e5e7a1509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights('a'+str(a)+'_n'+str(n)+'test.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
